{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非同期推論、同時推論を使った高スループットアプリケーションの基礎を学ぶ\n",
    "このプログラムでは非同期推論(asynchronous inferencing)を使い、かつ同時に複数の推論要求を実行することにより高い推論スループットを出すプログラムの基礎を学びます。  \n",
    "プログラムは引き続き、構成の簡単な画像分類(Classification)を題材としています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用しているシステムの構成を調べる\n",
    "最初に、ハードウエア構成を調べます。  \n",
    "`psutil`モジュールをインストールし、CPUのコア数を調べます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "!pip3 install psutil\n",
    "import psutil\n",
    "print('# of CPU cores = {}C/{}T'.format(psutil.cpu_count(logical=False), psutil.cpu_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "!pip install psutil\n",
    "import psutil\n",
    "print('# of CPU cores = {}C/{}T'.format(psutil.cpu_count(logical=False), psutil.cpu_count(logical=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力画像とラベルデータの準備\n",
    "まずは推論に使用する入力画像ファイルと、クラスラベルのテキストファイルをOpenVINOのdemoディレクトリからコピーしてきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "!cp $INTEL_OPENVINO_DIR/deployment_tools/demo/car.png .\n",
    "!cp $INTEL_OPENVINO_DIR/deployment_tools/demo/squeezenet1.1.labels synset_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "!copy \"%INTEL_OPENVINO_DIR%\\deployment_tools\\demo\\car.png\" .\n",
    "!copy \"%INTEL_OPENVINO_DIR%\\deployment_tools\\demo\\squeezenet1.1.labels\" synset_words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コピーしてきた推論入力の絵を表示して確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('car.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論に使用するIRモデルデータの準備\n",
    "推論に使用するモデルを`Model downloader`でダウンロードし、`Model converter`でIRモデルに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --name googlenet-v3\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/converter.py  --name googlenet-v3 --precisions FP16\n",
    "!ls public/googlenet-v3/FP16 -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "!python \"%INTEL_OPENVINO_DIR%\\deployment_tools\\tools\\model_downloader\\downloader.py\" --name googlenet-v3\n",
    "!python \"%INTEL_OPENVINO_DIR%\\deployment_tools\\tools\\model_downloader\\converter.py\"  --name googlenet-v3 --precisions FP16\n",
    "!dir public\\googlenet-v3\\FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "ここからPythonプログラム本体となります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プログラムで使用するモジュールをインポートする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスラベルテキストデータを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [ s.replace('\\n', '') for s in open('synset_words.txt').readlines() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Engineオブジェクトを生成する\n",
    "- Inference Engineコアオブジェクトを生成\n",
    "- モデルデータの読み込み\n",
    "- 入出力ブロブ(バッファ)の情報取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Engineコアオブジェクトの生成\n",
    "ie = IECore()\n",
    "\n",
    "# IRモデルファイルの読み込み\n",
    "model = './public/googlenet-v3/FP16/googlenet-v3'\n",
    "net = ie.read_network(model=model+'.xml', weights=model+'.bin')\n",
    "\n",
    "# 入出力blobの名前の取得、入力blobのシェイプの取得\n",
    "input_blob_name  = list(net.input_info.keys())[0]\n",
    "output_blob_name = list(net.outputs.keys())[0]\n",
    "batch,channel,height,width = net.input_info[input_blob_name].tensor_desc.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) `Query` API\n",
    "Inference engineにはQuery APIがあります。キーを指定することでプラグインの情報を取得することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ie.get_metric('CPU', 'RANGE_FOR_ASYNC_INFER_REQUESTS'))\n",
    "print(ie.get_metric('CPU', 'RANGE_FOR_STREAMS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plugin configuration\n",
    "`set_config()` APIを使うことによってInference engineプラグインの設定を行うことが可能です。  \n",
    "`CPU_THREAD_NUM`, `CPU_BIND_THREAD`, `CPU_THROUGHPUT_STREAMS`などの値をハードウエアに合わせて適切に設定することでパフォーマンスが上がる場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.set_config({'CPU_THREADS_NUM'       : '0'  }, 'CPU')  # default = 0\n",
    "ie.set_config({'CPU_BIND_THREAD'       : 'YES' }, 'CPU')  # default = YES\n",
    "ie.set_config({'CPU_THROUGHPUT_STREAMS': '1'   }, 'CPU')  # default = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルをIE coreオブジェクトにロードする\n",
    "読み込んだネットワークオブジェクトをInference engineプラグインにセットします。  \n",
    "この時、`num_requests`で推論要求オブジェクト(`infer_request`オブジェクト)をいくつ生成するか指定します。  \n",
    "`infer_request`の数だけ同時に推論要求を投げることが可能になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_net = ie.load_network(network=net, device_name='CPU', num_requests=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コールバック関数をセットする\n",
    "作成した`infer_request`ひとつづつ個別にコールバック関数を設定します。コールバック関数にラムダ式(無名関数)を指定することも可能です。  \n",
    "今回コールバック関数の中では推論結果に対して特に何も処理をしていません。単に推論終了カウントを増やす処理だけをしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_infer=0\n",
    "\n",
    "def callback(status_code, output):\n",
    "    global total_infer\n",
    "    total_infer  += 1\n",
    "\n",
    "for req in exec_net.requests:\n",
    "    req.set_completion_callback(callback, req.output_blobs[output_blob_name].buffer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論入力データを準備する\n",
    "推論入力画像を読み込み、入力ブロブのシェイプに合わせて変形します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('car.png')\n",
    "img = cv2.resize(img, (width,height))\n",
    "img = img.transpose((2, 0, 1))\n",
    "img = img.reshape((1, channel, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **非同期、同時推論**を実行する  \n",
    "- 推論実行 (非同期推論で400回実行)\n",
    "- 推論終了待ち\n",
    "- パフォーマンスデータおよび推論結果の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python APIのバグのワークアラウンド。本来不要。一度全`infer_request`に対してダミーの推論を実行する\n",
    "for req in exec_net.requests:\n",
    "    req.async_infer(inputs={input_blob_name: img})\n",
    "\n",
    "infer_slot = 0\n",
    "total_infer= 0\n",
    "max_infer = len(exec_net.requests)\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# 推論本体 400回推論を実行\n",
    "while total_infer<400:\n",
    "    req = exec_net.requests[infer_slot]\n",
    "    status = req.wait(0)\n",
    "    if status == 0 or status==-11:   # infer_requestのステータスが0(OK), -11(INFER_NOT_STARTED)なら推論要求実行\n",
    "        res = req.async_infer(inputs={input_blob_name: img})\n",
    "    infer_slot = (infer_slot+1) % max_infer\n",
    "\n",
    "# 全推論要求が終了するまで待つ\n",
    "for req in exec_net.requests:\n",
    "    while req.wait()!=0: pass\n",
    "\n",
    "# パフォーマンス表示\n",
    "total=time.time()-start\n",
    "print('max_infer={} time={:.4}sec fps={}\\n'.format(max_infer, total, total_infer/total))\n",
    "\n",
    "# 推論結果表示\n",
    "for i, req in enumerate(exec_net.requests):\n",
    "    output = req.output_blobs[output_blob_name].buffer[0]\n",
    "    idx = np.argsort(output)[::-1]\n",
    "    print('infer_request ', i)\n",
    "    for i in range(5):\n",
    "        print(idx[i], output[idx[i]], label[idx[i]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## ここまでで効率の良い推論プログラムの作り方について学びました\n",
    "ここでのポイントは下記のものになります。\n",
    "- 非同期推論を行う\n",
    "- デバイスの特性に合った同時推論数分、推論要求を投げてデバイスを飽和させる\n",
    "\n",
    "今回、`CPU_THREAD_NUM`, `CPU_BIND_THREAD`, `CPU_THROUGHPUT_STREAMS`などをデフォルト値と同じ設定で実行しました。    \n",
    "しかし、これらのパラメータや`num_requests`を使用するプロセッサの構成に合わせて最適化することでさらにパフォーマンスアップすることが可能です。  \n",
    "パラメーターを調整し、最高のパフォーマンスが出せる設定を狙ってみてください(**DevCloudで実行している場合、うまく設定すると2倍以上速くなる可能性があります**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<おわり>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~cpp\n",
    "enum StatusCode : int {\n",
    "  OK = 0, GENERAL_ERROR = -1, NOT_IMPLEMENTED = -2, NETWORK_NOT_LOADED = -3,\n",
    "  PARAMETER_MISMATCH = -4, NOT_FOUND = -5, OUT_OF_BOUNDS = -6, UNEXPECTED = -7,\n",
    "  REQUEST_BUSY = -8, RESULT_NOT_READY = -9, NOT_ALLOCATED = -10, INFER_NOT_STARTED = -11,\n",
    "  NETWORK_NOT_READ = -12\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "c9449627750e98e9b1ad30fb225936bb035c6d0e9c862047946b304a9e5cb973"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}